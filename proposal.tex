\documentclass[12pt,a4paper,titlepage]{scrreprt}
\usepackage{url}
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\usepackage{fullpage}
\usepackage{amsmath,amssymb}
\usepackage{graphics,graphicx,wrapfig,float,epsfig,subfigure,sidecap}
\usepackage{url}
\usepackage{svg}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[margin=0.45in]{geometry}
\usepackage[utf8]{inputenc}

\usepackage{color}
\addbibresource{OnlineExperiments.bib}

\begin{document}

    \title{PsychoCoffee}
    \subtitle{Open, Robust, Experimenter-Friendly Data Collection via the World Wide Web}
    \date{\small{{rtibbles}@ucsd.edu}}
    \author{{\bf Richard Tibbles} \\ \\
                \small{University of California, San Diego} \\
                \small{Department of Cognitive Science} \\
                \small{Center for Human Development}}
    \maketitle
\newpage

\tableofcontents

\newpage

\chapter{Introduction}
    
    The basic question of Cognitive Science is to understand the function and instantiation of the human mind. The investigation of such phenomena thus far in the history of Science has been highly focused on the minds of individuals in very particular social contexts. In contrast to the physical sciences, where the particular spatial location of experiments is assumed to have no bearing on the fundamental laws being investigated (and more strongly, were it to do so, these would not be 'fundamental' laws as we understand them), it is at the very least an open question in psychological sciences the extent to which the social context affects the phenomena under investigation. In addition to social context, the population of minds in which psychological investigations are generally undertaken is also considerably limited as compared to the broad swathe of humanity.

    In addition to a bias in the sampling of minds, and the contexts of minds, there is insufficient replication of positive results in the literature. Replication is a time consuming process, requiring an experimenter to recreate an experiment conducted by a colleague. However, if there is no obvious extension of the experiment, the incentive for this replication will be very low, as no original results can be further tested in the replication of an older experiment. In addition, the original positive result may be very closely tied to certain features of the experimental setup - while it is desirable that results be robustly replicable, it is also of interest to discover the relevant reasons why results do not replicate. Finally, direct replication, whereby the repetition of the experiment in all relevant details is undertaken in a different laboratory, by different researchers, is incredibly rarely done, and is not incentivized.

    In order to provide a tool to contribute one part of a range of possible solutions to the above problems, a framework for online experimental testing is proposed. An online experimental system has the advantage of being able to reach a wider range of potential participants, allow for the gathering of larger datasets with lower experimenter effort, and the rapid replication of existing experiments, through the sharing of experimental materials. While similar systems are already in development and use, they lack several key features to help tackle the problems outlined above. To this end, the proposed system, PsychoCoffee, will be: open source, integrate features and defaults for open sharing and alteration of experimental code, and scaffold creation of experiments with a graphical user interface.

    The system will be open source in order to allow for inspection of the source code to verify, modify, and enhance the system for particular experimental needs. Developing software as open source is in line with the Scientific demand for clarity and disclosure of experimental methods. Without the ability to inspect the source code of experiments and the experimental framework, claims about the system are not amenable to independent verification.

    Leveraging existing mechanisms for sharing and distributing code will allow experimenters to rapidly replicate, modify, and extend colleague's experiments, in order to test the range of replication of experiments, and to investigate additional aspects of phenomena. Finally, the use of a graphical user interface to define the elements and control flow of an experiment will allow broader and more efficient participation in experimental design and creation, in a time when programming is still a much sought after skill in Social Science laboratories.

\newpage

\chapter{Related Work}
\section{Open Science}
\subsection{The Role of Openness in Science}

Openness is core to the historical and current process of Science. \textcite{boulton_science_2012} identify much of the progress in Science over the preceeding centures as a result of the open sharing of observations and theories, which has allowed replication, refinement, and rejection. This has allowed Science to operate as a self-correcting distributed process across researchers, institutions, and countries. Without open sharing of research findings in journals, and the dissemination prior to publication that occurs in peer review, the Scientific process as currently instituted would grind to a halt.

However, openness itself is a continuum, with current practices falling somewhere in the middle. Some journals are open access, requiring no subscription, others are locked behind a paywall (generally accessible to those at research institutions, but not in all cases). The sharing that happens in these publications is not always completely open. Methods sections frequently fail to meet the requirement of allowing for reproducibility as described by \textcite[pp.~24-25]{popper_logic_1959}: \begin{quote} Indeed the scientifically significant physical effect may be defined as that which can be regularly reproduced by anyone who carries out the appropriate experiment in the way prescribed. No serious physicist would offer for publication, as a scientific discovery, any such ‘occult effect’, as I propose to call it — one for whose reproduction he could give no instructions.\end{quote} In addition, \citeauthor{boulton_science_2012} identify this lack of open sharing of materials and data as a key need for change in current Scientific practices - with a need for institutional incentives for openly sharing data and study materials required to verify and replicate claims in published studies.

It would seem that Science is already a fairly open enterprise, therefore, and august institutions such as the Royal Society \parencite{boulton_science_2012}, and the National Academy of Sciences \parencite{committee_on_science_engineering_ensuring_2009} have made recommendations about how institutions can incentivize Open Science among their researchers. Is the sharing of data and materials they urge sufficient to allow for review, replication, refinement, and rejection of scientific observations and theories?

\subsection{Closed Source is not Open Science}

While some of the initiatives in the institutional reports involve the creation of Open Source software to support research, neither reports make any claims that Open Source Software is a necessity for Open Science. Software is considered Open Source if the source code is freely available to other users, and able to be modified without restriction. In the case of code for scientific experiments and analyses, this means that not only can the experiment or analyses be directly replicated by another researcher, but also that code can be modified, so that the analyses or experiments can be extended beyond the original scope of the research.

This explanation encapsulates much of the impetus for Open Source in Open Science - by allowing the quick replication and extension of experiments and analyses, Science can fulfil its promise of accumulation of knowledge over time. The possibility of growth of knowledge, whether by elimination of theories, paradigms, research programmes (or by any means) seems to be central to almost all conceptions of how Science is, regardless of the specifics of one's Philosophy of Science  \parencite{popper_logic_1959,kuhn_structure_1962,lakatos_criticism_1970,feyerabend_science_1978}.

Open Source science has two important strands: publishing code, and publishing data \parencite{stodden_reproducible_2010,stodden_trust_2011}. Both are important for being able to rerun analyses. Code is frequently withheld because of the difficulties of properly documenting code for publication, or due to the fact that many scientists are not trained computer programmers, and hence may feel nervous about showing their 'novice' programming to the world \parencite{barnes_publish_2010}, there may also be concerns about the intellectual property rights associated with the code \parencite{mccafferty_should_2010}, although a study of the issuing of patents for published research has shown a dampening effect of patent issuance on citations of the associated papers \parencite{murray_formal_2007}, suggesting that stringent protection of intellectual property may prevent research from being more broadly influential on a field. 

Restricting access to code also increases the burden on the community of researchers - meaning that much work will likely not be replicated and hence not be validated as reproducible something that the journal Science is starting to recognize as important \parencite{mcnutt_reproducibility_2014}. Ultimately, restricting access to code used in experiments and analyses protects individual papers from appropriate scientific scrutiny, and impedes the growth of scientific knowledge, and almost any code will be more illuminating than no code at all \parencite{barnes_publish_2010,mccafferty_should_2010}.

In addition to open code, open data is important for the proper scrutiny of scientific findings. In fact, this has been the policy of the American Psychological Association \parencite{association_publication_2001} since at least 2001. However, obtaining this data, in an increasingly digital world has, empirically, become more difficult (in spite of the ease of digital communication). Compared to a study conducted in 1973 when authors of a reanalysis study were able to obtain 75\% of relevant data sets, \textcite{wicherts_poor_2006} were only able to obtain 27\% of data sets from authors published in an APA journal. While concerns, as with code, about intellectual property, and scrutiny that might overturn individual research findings, apply to the open release of data \parencite{wicherts_publish_2012}, the advantages of increased citation of primary papers that release data, as well as the possibility for more rigorous scrutiny, and secondary analyses makes a strong imperative for open sharing of data sets.

A case study in pharmacological research helps to show the advantages of this kind of open research, by openly posting data as they were generated, experts were able to identify themselves and work on the data as soon as it was released, giving much greater research speed \parencite{woelfle_open_2011}. In addition, by opening the laboratory process and notebooks to the world, the research was more accountable to the public funding the research through taxation, and the research became less personnel dependent - the graduation of a graduate student would no longer result in the unsatisfactory termination of a research project.

Finally, in addition to open sharing of scientists' data and code - entire open source software projects for scientific purposes have seen considerable success, particularly in Cognitive Science. Freesurfer, a widely used tool for analysis of neuroimaging data is maintained as part of a larger research project \parencite{fischl_freesurfer_2012}. Software for running psychophysical and psychological experiments PsychToolbox \parencite{brainard_psychophysics_1997} and PsychoPy \parencite{peirce_psychopypsychophysics_2007,peirce_generating_2008}, are available for the closed source MATLAB and Open Source Python programming languages, respectively. EEGLAB is another widely used MATLAB package developed as Open Source for the analysis of single trial EEG data \parencite{delorme_eeglab:_2004}. Lastly, Chronoviz \parencite{fouse_chronoviz:_2011} is a tool developed for the analysis of time coded data - allowing for easier and more efficient multimodal analyses of complex data sets. By freely sharing the software and source code, these research tools are capable of being extended by researchers around the world, and useful additions and modificiations shared back with the larger research community. As such, far more powerful tools than can be built in isolation by single laboratories are possible.

\section{Replicability and Generalizability}

\begin{quote}
Only when certain events recur in accordance with rules or regularities, as is the case with
repeatable experiments, can our observations be tested — in principle — by anyone. We do not take even our own observations quite seriously, or accept them as scientific observations, until we have repeated and tested them. \parencite[p.~23]{popper_logic_1959}
\end{quote}

Although there have been concerns for nearly forty years \parencite{greenwald_consequences_1975} about the truth and generalizability of published research in Psychology, the coining of the \textit{replicability crisis} \parencite{pashler_editors_2012} seems to have catalyzed action on behalf of at least some actors in the field. The root of much of the current concern was first named as the file drawer problem in the last century \parencite{rosenthal_file_1979}. The initial problem is simply articulated - in the extreme case, the bias for publishing positive results means that at an alpha criterion for significance of 0.05, a researcher's file drawer could contain nineteen failed experiments that failed to show statistical significance for every one experiment that showed statistical significance, the net result being that the evidence for the effect supported by the one published study is no better than chance.

To compound the file drawer effect, verification of positive results is frequently via conceptual, rather than direct replication \parencite{pashler_is_2012}. This has the result that verifications can be taken as evidentially supportive of the original publication, whereas falsifications can be dismissed as operationally different, and hence uninformative to the evidential basis of the original result. In addition, surveys of the literature have found that very few direct replications of positive research findings occur \parencite{makel_replications_2012} - in this way, a result that, by chance, was deemed significant can be the basis for bootstrapping an entire research program through conceptual replications based off a single finding that may lack evidential basis.

\subsection{Meta-Analyses of Reported P-Values}
The extent of the problems in the current literature can be quantified using meta-analytic methods. While the methods themselves cannot, of course, indicate the truth or falsehood of a particular study finding, analysis of the distributions of statistical values can help to highlight research practices that will tend to inflate the statistical significance of a phenomenon \parencite{john_measuring_2012}. These practices include: unreported exploratory analyses (doing analyses until a significant p-value is found, but only reporting the significant analyses), stopping the experiment when a significant result is found, and removing outliers after initial analyses have been run - these will all increase the effective alpha criterion for significance of the effect reported, as more comparisons have been conducted than have been reported. These strategies will all tend to exacerbate the effect of the file drawer problem, creating even more studies that are likely to be false positives \parencite{bakker_rules_2012}.
Fortunately, there is some hope for detecting these methodological biases through meta-analytic statistical tests of the distribution of p-values, in an attempt to highlight possible instances of \textit{p-hacking} \parencite{simonsohn_p-curve:_2013}. \textit{P-hacking} is used to describe the above practices (and others) that might result in the inflation of the number of significant p-values in the literature.

To detect \textit{p-hacking}, it must be first recognized that there will be a highly biased sample of p-values in the literature, due to publication biases for positive effects. However, a null effect that has been reported without \textit{p-hacking} will have a distribution of p-values proportional to the p-values themselves - that is a p-value of 0.045 would have a probability of 0.045 of occurring, this can then be rescaled to take into account the bias towards reporting results with p-values of less than 0.05. A real effect, however, will have a distribution of p-values that is skewed towards lower p-values, as a real effect will more frequently appear to be distributed differently from the null at greater than the alpha criterion for chance.

By contrast, reported p-values in the literature that are the result of \textit{p-hacking} will have a skew towards lower p-values, as their criterion for appearing in the literature is simply being less than the alpha criterion of 0.05. Under conditions of \textit{p-hacking}, the process will halt as soon as a result just less than 0.05 is reached, and so the distribution will be skewed towards the upper end. This could have the net result of \textit{p-hacking} on real effects resulting in an appearance of null results - it can only be hoped that there are enough sufficiently powered studies in the literature to counteract any results of \textit{p-hacking} for real effects.

\subsection{Endemic Lack of Replication}

A crude but effective survey of the existing literature searched for articles that contained the search term "replica*" \parencite{makel_replications_2012}. In order to assess the correlation of the positive match of the search term with actual replication of experimental results (rather than simply proposing replication or some other phrase that would match), a sub-sample of 500 articles was inspected by the researchers. 68\% of these were reported as actual replications (whether conceptual or direct), and this scaling was used to attenuate the reported results for percentage of articles that were replications.

After this scaling, 1.07\% of all articles were found to be replications, with a trend over time towards a slight increase in replications in the recent past. However, the trend for replications was towards conceptual, rather than direct replications, over time, and towards authors of the studies being replicated doing the replications. Independent, and more generalizable replication of experimental results appears to have reduced.
While the majority of replications were successful, there was a significant difference in replication success rate if the replication was conducted by someone other than the original study's authors.

One encouraging finding of the study is that the median citation count for the replications was 17, while significantly lower than the median citation count of the articles being replicated (64.5), it was significantly higher than the median citation count for the journals surveyed. This suggests that to some extent the replication of results is valued by the scientific community.
\subsection{Generalizability of Results}
\begin{quote}Some say that psychological science is based on research with rats, the mentally disturbed, and college students. We study rats because they can be controlled, the disturbed because they need help, and college students because they are available. \parencite{birnbaum_psychological_2000}
\end{quote}
While it is obvious to practising researchers that most participant pools are composed of very limited populations \parencite{buchanan_using_1999,birnbaum_psychological_2000,kraut_psychological_2004,birnbaum_human_2004,buhrmester_amazons_2011}, the classification of the majority of behavioural experiment participants as Western, Educated, Industrialised, Rich, from Developed countries (or WEIRD), and the highlighting of disparities in performance on canonical psychological tasks between these participants and a broader selection of humanity  \parencite{henrich_beyond_2010} has helped to bring the limitations of these populations into sharp focus. While the strong thesis of Henrich et al. that the most frequent participants in behavioural experiments (i.e. American undergraduate students) are outliers in human psychology goes too far \parencite{bennis_weirdness_2010}, it is generally acceptable at least that claims of universality of behavioural phenomena from this sample go too far \parencite{baumard_weird_2010,ceci_weird_2010,konecni_responsible_2010}.
\subsection{Improving Replication and Generalization}
While statistical meta-analytical techniques can indicate likely instances of methodological issues, to verify or falsify reported effects, both a guarantee of confirmatory research, and direct replication are needed. Several strategies for this have been suggested. Scientists could be incentivized to replicate studies, by offering joint publication with the original study \parencite{koole_rewarding_2012}, creating metrics and crowdsourcing replication \parencite{nosek_scientific_2012}, creating an open collaborative framework for sharing replication efforts, materials, and results \parencite{collaboration_open_2012}, and using undergraduate research projects (both independent research projects, and in project based learning classes) as a means of conducting direct replication \parencite{grahe_harnessing_2012,frank_teaching_2012}. Additionally, the open sharing of data, materials, and workflow \parencite{giner-sorolla_science_2012,nosek_scientific_2012,miguel_promoting_2014} are all highlighted as ways to open up both the experimental process and experimental results to proper scrutiny and replication. Effort to replicate is also a significant barrier, which the use of online data collection can help to overcome \parencite{grahe_harnessing_2012}.

Finally, online experiments can help to broaden the participant pool \parencite{gosling_wired_2010} - unfortunately, due to the current global distribution of Internet access, mostly concentrated in industrialized countries, and with the affluent of more rapidly developing ones, opportunities will be restricted to discover behavioural phenomena that are robust across industrialized cultures, and more affluent members of rapidly developing societies.
\section{Online Experiments}
One way, therefore, to broaden the reach of psychological research beyond the participants immediately available in the vicinity of the laboratory is to use Internet based studies to recruit a far broader and heterogeneous population. In early web studies, the lack of widespread Internet penetration led to concerns of repeating the same population difficulties shown in undergraduate recruitment - however, in a range of areas of study, from decision making to social psychology, experiments successfully replicated the results of laboratory based studies, while still attracting a more diverse sample than would ordinarily be the case \parencite{krantz_comparing_1997,buchanan_using_1999,birnbaum_decision_2000,mcgraw_integrity_2000,gosling_should_2004,ritter_internet_2004}. Further, the development of this new methodology quickly led to the formulation of best practices to overcome some of the perceived issues of confidentiality, reduced experimental control, multiple submissions, incomplete submissions, dropout, and misunderstanding instructions \parencite{reips_web_2000,reips_standards_2002,birnbaum_human_2004}. While it is not always the case that study materials used in laboratory experiments can be used identically online and yield the same results \parencite{buchanan_nonequivalence_2005}, the web provides an intermediary point between the laboratory and field studies to test the generalizability of experimental procedures and findings to less controlled contexts. It can also reduce experimenter bias and demand characteristics, and allow for experimental measures between subjects of motivational effects of studies through comparisons of drop out rate \parencite{reips_web_2000}, which is less measurable in laboratory experiments due to the pressure from the social context of the experiment, and frequently the desire for class credit.
\subsection{Current Applications}
[Frye: This section is going to be really strong for your work. You kind of sideways mention a bunch that the online studies have been working in a way that is useful and generalizable, but a dedicated section saying that explicitly will be really useful]
[Section exploring range of experiments implemented online.]
\subsection{Current Practices}
[Needs more][Frye: Yeah, in the part you end up adding, make sure to discuss how this type of incentivizing might bias not only the results, but finding significant results at any cost. It would be a shame to use TV to get a null result, so I bet they always find something]
Many participants for online experiments are now paid, similarly to many laboratory studies, particularly through the use of crowdsourced labour platforms such as Amazon's Mechanical Turk \parencite{buhrmester_amazons_2011,rand_promise_2012,crump_evaluating_2013}. However, other work has relied entirely on voluntary participation, with significant boosts to participation coming from media coverage (and in the case of data gathered through the BBC UK Lab, through cooperation with the BBC television show \textit{Bang Goes the Theory}) \parencite{owen_putting_2010}. In addition, results relying entirely on voluntary participation have been able to produce results very similar to lab results, with more diverse populations, across age range, educational, and socio-economic background \parencite{germine_is_2012,halberda_number_2012}.
\subsection{Challenges for Online Experiments}
[Needs more][Frye: What about monitor refresh rates and some psychometric studies?]
Intermittent connectivity, high latency, and variable participant hardware can pose significant challenges for online experiments. While precautions are taken to preload experimental materials to limit the impact of latency, even very recent and popular platforms for online experiments, such as testmybrain.org \parencite{germine_is_2012, halberda_number_2012} will fail if Internet connectivity is lost at critical moments in the experimental flow. This is a particular challenge when attempting to reach a more diverse population of participants, as this will also involve a more diverse range of Internet connections - both in terms of speed and reliability.

Due to variability in participant hardware, and difficulties of controlling stimulus presentation through the web interface, very short stimulus presentation times, on the order of 64ms and below, have not been able to be used so far in web based experiments \parencite{crump_evaluating_2013}. Further, participant owned hardware is highly unlikely to be as optimized for rapid stimulus presentation and high fidelity response time recording. In addition to thresholding the lower limits for length of stimulus presentation, it is also important to understand the characteristics of different participant devices, and their accuracy with regards reaction time measurements.
\section{Experimental Implementation}
\subsection{Current Practices}
[Needs more - ePrime, Presentation, PsychToolbox, etc.]
The proposed platform, PsychoCoffee, is modelled after another success in open source software for Psychological research - PsychoPy \parencite{peirce_psychopypsychophysics_2007,peirce_generating_2008}. PsychoPy is a platform for generating Psychophysical and Psychological experiments. By leveraging a powerful, multipurpose, cross-platform language like Python, PsychoPy allows researchers to develop code that will run in the same way across multiple computers and operating systems. In addition, because the framework itself is Open Source, more technically minded experimenters are able to add new features that they need for their experiments. Using this framework, PsychoPy has achieved the difficult dual task of creating a stable but customizable platform. In addition, due to the focus of the Python language itself on readability of code \parencite{_python_????}, experiments coded using PsychoPy are amenable to inspection by other scientists, seeking either to verify, replicate, or extend research findings. As such, PsychoPy goes a long way in assisting Psychology in creating open experiments to increase scrutiny and replication, also, due to the cross-platform (and free) nature of the underlying language, the bar for replication is lowered considerably.[Frye: So why change from PsychoPy? I think in this section some of the limitations of what's currently around and how that leaves problems that need solving is going to set you up for actually moving in your experiments and the design of PsychoCoffee]
\subsection{Challenges for Implementing Experiments}
[Frye: Ah, ok. So this is a good start, but I think you need a bit more on why the current state is unsustainable and not solving the problems you do such a good job of laying out earlier. Also, maybe title this section Current Challenges? Since, you know, you're fixing them!]
Issues still remain, however, as the extra step of sharing experimental code, no matter how readable it naturally is, remains a barrier to openness. In addition, the difficulty of replication in participant recruitment, use of laboratory space, and personnel remains a barrier. To address this, PsychoCoffee will use coffeescript, a readable Python-inspired form of the Javascript programming language to maintain the accessibility of PsychoPy, while creating an experimental platform designed for the Web.

While many Psychology laboratories are already using the Web for experiments, and have created their own frameworks within which to create their experiments \parencite{krantz_comparing_1997,germine_is_2012}, no platform is yet available for general purpose programming of online experiments. One program in development is Cognilab \parencite{_cognilab_????}, run by a private company startup, with no obvious plans to open source their software. Success of this software would do little to open up the processes of scientific experiments to scrutiny and inspection as required by the foregoing concerns about the state of Psychology.
[Frye: I think you could be more explicit on the weaknesses. Just call people out, haha, and be the super hero that fixes them]
\section{Supporting Programming}
The problem of supporting people in learning to program has come about in synchrony with the rise of the microcomputer \parencite{papert_teaching_1971,papert_teaching_1971_1,papert_computer_1971,papert_twenty_1971,papert_making_1972}. However, in spite of the early impetus to teach programming skills to younger children, undergraduate introductory programming education still produces highly mixed results - a widely reported bimodal distribution in grades \parencite{dehnadi_camel_2006} \parencite{robins_learning_2010}, and failure rates that vary widely \parencite{bennedsen_failure_2007}, but with a mean at around thirty percent. Attempts to teach introductory programming as a general education requirement, rather than only to computer science majors has seen results comparable to failure rates in computer science courses alone \parencite{guzdial_education:_2009}, however, when students for whom programming requirements were atypical were analyzed separately, their failure rates were closer to fifty percent. Reacting to these problems in learning to program, Scratch \parencite{maloney_scratch:_2004}, and Alice \parencite{moskal_evaluating_2004} have both been created as more graphical ways to introduce students to programming - Scratch is designed for K-12 education \parencite{malan_scratch_2007}, while Alice has been focused on introducing programming concepts to undergraduates and then transferring understanding into existing programming languages \parencite{dann_mediated_2012}.

Notably, Khan Academy \parencite{_computer_????} has launched a portal to learn computer programming, based on Processing - a language designed to introduce programming through interaction with animations and visual elements, much like Scratch \parencite{peppler_supergoo_2007}. However, this portal has come under criticism \parencite{victor_learnable_2012}, due to its failure to scaffold the learning process. These recommendations are made as a way to support novice programmers in understanding a language:
\begin{quote}
\begin{itemize}
\item The environment can make flow tangible, by enabling the programmer to explore forward and backward at her own pace.
\item The environment can make flow visible, by visualizing the pattern of execution.
\item The environment can represent time at multiple granularities, such as frames or event responses, to enable exploration across these meaningful chunks of execution.
\end{itemize}
\end{quote}
\includegraphics[scale=0.4]{Scratch_Layout}

At least one of these recommendations is at least partly realized in Scratch where the pattern of execution is made visible by the labelled interlocking execution steps. Allowing for video editor like \textit{scrubbing} of the execution result, and allowing for key frames to be highlighted either at fixed time intervals or indexed to events in the execution would implement the other recommendations.
\chapter{Research Questions}
\begin{itemize}
\item How to design an experimental platform to be scalable, robust, and collect reliable experimental data across a range of user devices?
\item How to design a system that encourages open sharing of experimental code and materials, to allow for rapid replication and extension?
\item How to design a system to support rapid prototyping and development of psychological tasks?
\end{itemize}
\chapter{Proposed Research}
\section{System Design}
[Frye: So I think when this gets written in more detail, a little background section detailing why this is important to your overall goal of creating a service that is more openly accessible and generalizable would be good. It's ok if it reiterates some of your early intro, because I think you really want to drive home the importance of each of these as a step]
\subsection{Open Source}

The system will use the existing code versioning tool, git \parencite{_git_????}, and the open source code sharing platform github \parencite{_github_????}, during the experimental implementation process. This will allow the quick sharing, modification, and extension of experiments created within it. As a default in the system, these materials will be openly licensed, and free to be distributed. Using these existing tools to share experimental code and materials will enhance the capacity to replicate and interrogate the methods used by behavioural science in gathering data \parencite{giner-sorolla_science_2012,nosek_scientific_2012,miguel_promoting_2014}. In addition, the use of a versioning system, such as git, allows for modifications and extensions to experiments to be tracked - both across time and across researchers. By examining the task demands across these different changes, and subsequent behavioural measures, researchers can start to collectively define the extent of generalizability of findings, based on the extent of replication given different modifications and extensions.

By leveraging the resources of these existing Open Source tools, experimental code will be freely shared, modifiable, and the differences between different versions of experiments will be readily inspectable using widely used tools.
\subsection{Extensible Framework}
The platform would use a modular approach to stimuli presentation and response capturing. By defining programmatic base classes of stimulus and response, the software can be incrementally developed by the original developers, and later by the broader community, to suit varied experimental needs. 
\subsection{GUI Based Programming}
In addition to creating an open framework, the platform will have a graphical user interface for experiment design, modelled after the PsychoPy Builder (a timeline based view of experimental components that can be manipulated without any knowledge of the underlying Python code), and the MIT Scratch programming language \parencite{resnick_scratch:_2009}. This will help to lower a further barrier to replication, allowing the platform to be used in undergraduate replications of existing research findings  \parencite{frank_teaching_2012}, and also letting those with less programming experience have a scaffolded programming experience to create the flow of their experiments.\\
\includegraphics[scale=0.4]{edit_interface}\\
\section{Problem Finding}
\subsection{User Surveys}
User surveys will be used to gather more in depth information about current practices in the research community around online data collection than is available in literature sources.
\subsection{Participant Observations}
Users using current tools for creating online experiments (such as modifications to Qualtrics, custom programming on Amazon Mechanical Turk, Google Forms, SONA surveys etc.), will be observed in order to understand current pain points in developing online experimental materials.
\section{Prototyping}
\subsection{Beta Testing with Limited Capabilities}
First pass implementation with a bare bones set of features - visual stimulus presentation, keyboard response input, and timing features. Drag and drop GUI for sequencing stimuli and setting up contingencies between stimuli and response objects.
\subsection{Activity Tracking and User Feedback}
Tracking experimenter activity to understand how experimenters use software. In line feedback for comments, suggestions, and to motivate enhancement of feature set and user interface design.
\section{Evaluation}
\subsection{Automated Testing}
The design of the system will be subjected to automated testing using PhantomJS \parencite{_phantomjs_????} to test across platforms - both for experiment design GUI, but also some sample experiments. The Stroop task, Flanker task, and a coherent dot paradigm will be included in the test scenarios for automated testing. The automated mock experimental sessions  will be recorded and the subsequent playback examined to check for visual, auditory, and timing congruity between different browser/OS combinations. This will help ensure that visual, auditory presentation are the same across different platforms, and also that the timing of experiments does not differ due to client side software differences.
In addition to automated testing on mock systems, automated benchmarking on real end user systems will be conducted. Users will be asked to navigate to the site, which will then do a range of automated click and navigation events at various timing intervals. The speed of response of the web application to these activities will then be used to build a profile of the responsivity and accuracy of varied hardware configurations to the system. It is assumed that newer Browser and Operating System combinations will be indicative of newer hardware, and hence should have the best responsivity to changes in application state - however, understanding the range of variation of software responsivity will help to set a lower bound for the accuracy in timing that can be achieved by the system.
\subsection{Human Testing}
Even though a significant advantage of online experiments is the ability to find signal in noisier data due to the far higher number of participants \parencite{birnbaum_web-based_2001}, it is important to understand the performance characteristics of a web based system, in order to place limits on the kind of effects that can be measured. 

Stimulus presentation sensitivity can be measured by a very simple procedure whereby increasing stimulus presentation durations are presented to the participant, on either the left or the right of the screen - the participant will then press either left or right when they see the stimulus. By finding performance at chance, the threshold of visibility of stimulus duration will be determined. These thresholds can then be compared to the device characteristics recorded by the web browsers, which include browser version and operating system, allowing information to be gathered about the performance characteristics of certain kinds of devices.
Reaction time sensitivity will be ascertained by a two step process. Firstly, the above automated testing will set a lower bound on the reaction time sensitivity achievable across a range of client side platforms. Secondly, user responses can be recorded during reaction time tasks. By aggregating over users with similar hardware set ups, differences in ranges of response between different hardware configurations can give relative error comparisons for different hardware set ups. Finally, by comparison to more carefully controlled tests in the laboratory, more accurate mean user reaction time profiles on the tasks can be determined, and comparisons made between these and the remote data.
\subsection{Laboratory Comparisons}
Laboratory comparisons allow for more direct comparison of the characteristics of browsers. Using a background process that records all input to the test computer logged with the system clock time, events happening in the browser can later be synchronized with the recorded input events. This will allow trial by trial comparisons of user reaction times, giving a large quantity of within event comparisons. This will allow high confidence estimates of the variability between reaction times that would be recorded in an ordinary laboratory study (i.e. still with timing considerations from hardware interfaces) as opposed to those being conducted in an online study. Further, the variability measured in the laboratory comparisons (again, across a range of Browser and Operating System combinations) can then be used to estimate the additional contribution to response time variability caused by engaging with experiments in a non-laboratory setting.
\subsection{Implementation and Replication of Canonical Experiments}
This evaluation comprises two separate tests of the system. Firstly, asking motivated, but programmatically naive researchers (selected undergraduate research assistants) to use the software to replicate previously well validated experiments tests the capacity of the system to properly scaffold and support novice users in implementing experiments.

Secondly, by replicating the results of established experiments, it follows the pattern of confirmation and proof of concept pursued by previous work on the efficacy of online experiments \parencite{germine_is_2012,crump_evaluating_2013}, and will show the impact of the system itself, and its capacity to reproduce behaviour that has been observed  both in the laboratory and in online studies. As with previous work, comparing the noise and reliability of the data across laboratory and online studies of canonical experiments will give a further evaluation of the system's capacity to faithfully reproduce robust psychological effects online.
Contingent on the results of the benchmarking studies, it may be possible to attempt to extend the suite of test experiments to those requiring stimulus presentation times below the 64ms threshold previously identified \parencite{crump_evaluating_2013}. This would open up the possibility for conducting psychophysical experiments through the web browser.
\subsection{Real World Use}
The proof of the pudding is in the eating, and only through the adoption and use of the software by researchers pursuing their own research questions will the system truly be proven - both in terms of its ease of use, but also in terms of it achieving its avowed goals of enhancing openness of scientific materials and research, and promoting replication, modification, extension, and understanding of the range of generalizability of experimental results.
\chapter{Contribution}
\begin{itemize}
\item Provide tools to allow psychological science to collect data at scale
\item Provide tools that recognize the needs of psychological scientists to rapidly generate and deploy experiments
\item Provide tools that allow for quick replication and extension of existing experiments, in order to place psychological science on a surer empirical footing
\item Provide an open source tool that is open to inspection by peers, modifiable, and freely available
\end{itemize}
\printbibliography
\end{document}